{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60642a59b2c46ada5c9d8524d90ddd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16815b91aabd48de942fdb922382b0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  #\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_layers = len(model.model.layers)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(target_path, target_file, data):\n",
    "    if not os.path.exists(target_path):\n",
    "        try:\n",
    "            os.makedirs(target_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise\n",
    "    with open(os.path.join(target_path, target_file), 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_truncated_outputs.json\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "input_ids = tokenizer(queries, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genenerate default outputs.\n",
    "output_ids_default = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch_queries = queries[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch_queries, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        generations = model.generate(**input_ids, max_new_tokens=35, do_sample=False, use_cache=True)\n",
    "        output_ids_default.extend(generations)\n",
    "\n",
    "outputs_default = tokenizer.batch_decode(output_ids_default, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128009, 128009, 128000,   4897,   1847,    574,    264,   1972,  32095,\n",
       "          1481,   2058,      0,    358,   2846,  16089,    584,   1051,   3025,\n",
       "           311,   2586,    704,    389,   1948,    304,    279,    842,     13,\n",
       "           578,  13734,    574,   9249,     11,    323,    433,    574,    264,\n",
       "          2294,  16975,    311,    387], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids_default[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"generations/\" + model_name + \"/\"\n",
    "write_json(output_path, \"default.json\", outputs_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate steering vectors\n",
    "\n",
    "pos_p = \"Act like you are really happy.\" #\"[INST] Act like you are really happy [/INST]\"\n",
    "neg_p = \"Act like you are really sad.\"  #\"[INST] Act like you are really sad [/INST]\"\n",
    "\n",
    "contrastive_statements = [pos_p, neg_p]\n",
    "\n",
    "input_ids_contrastive = tokenizer(contrastive_statements, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "steering_vectors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states = model(**input_ids_contrastive, output_hidden_states=True)[\"hidden_states\"]\n",
    "    for layer in range(len(hidden_states)):\n",
    "        final_token_reps = hidden_states[layer][:, -1, :].float()\n",
    "        steering_vectors.append(final_token_reps[0] - final_token_reps[1])\n",
    "\n",
    "    steering_vectors = torch.stack(steering_vectors)[1:]  # Cutoff the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook(layer, alpha):\n",
    "    def hook(m, i, output):\n",
    "        v = steering_vectors[layer].to(output[0].device)\n",
    "        output[0][:, -1] += alpha*v\n",
    "        return output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = +.25\n",
    "steering_layers = [i for i in range(2, len(model.model.layers)-2, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = []\n",
    "\n",
    "### Apply steering hooks.\n",
    "with torch.no_grad():\n",
    "    for layer_id in steering_layers:\n",
    "        h = model.model.layers[layer_id].register_forward_hook(get_hook(layer_id, alpha))\n",
    "        handles.append(h)\n",
    "\n",
    "output_ids_steered = []\n",
    "\n",
    "### Generate steered outputs.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch_queries = queries[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch_queries, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        generations = model.generate(**input_ids, max_new_tokens=35, do_sample=False, use_cache=True)\n",
    "        output_ids_steered.extend(generations)\n",
    "\n",
    "    for h in handles:  # Remove hooks.\n",
    "        h.remove()\n",
    "        \n",
    "    outputs_steered = tokenizer.batch_decode(output_ids_steered, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That game was a real blast! I'm so glad we could celebrate together. I'm already looking forward to the next one! - 100% of the time! - 100\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_steered[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"generations/\" + model_name + \"/\"\n",
    "write_json(output_path, \"steered_alpha_\" + str(alpha) + \".json\", outputs_steered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
